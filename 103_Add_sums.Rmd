---
title: "103 Add sums"
author: "DHJ"
date: "2024-09-17"
output: 
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: inline
---

This script does the following things:  
* Reads data made by script 101  
* Deletes all sums made in script 101 before , i.e. "CB_S7" (incl. LOQ) accompanied by "CB_S7_exloq" (excl. LOQ)  
* Adds new sums defined in the top of script '101_Combine_with_legacy_data_functions.R', based on MGR's excel table in Sharepoint/project portal   
    - these suns are on the format "sumPCB7" (incl. LOQ) accompanied by "sumPCB7_exloq" (excl. LOQ)  
* Saved file will be used in script 105  

## 0. Settings  
```{r}

selected_year <- 2023

```


## 1. Load libraries and functions   
```{r, results='hide', message=FALSE, warning=FALSE}

library(dplyr)
library(purrr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(readr)

# Load self-made functions
source("002_Utility_functions.R")
source("101_Combine_with_legacy_data_functions.R")

# Function   
plot_from_searchtext <- function(search_text, ignore.case = TRUE, data = dat_all){
  pars1 <- dat_all %>% distinct(PARAM) %>% pull(PARAM)
  pars_sel <- grep(search_text, pars1, value = TRUE, ignore.case = ignore.case)
  dat_all %>%
    filter(PARAM %in% pars_sel) %>%
    count(PARAM, MYEAR) %>%
    ggplot(aes(MYEAR, PARAM, fill = n)) +
    geom_tile() +
    labs(title = paste("search_text =", sQuote(search_text)))
  
}

```


## 2. Get data  

### a. Main data  
Read and reformat the most recent data (by default)  
```{r}

# Files 
files <- list_files("Data", pattern = "101_data_updated")

# Pick file
filename <- files[1]
cat("\nLast file:", filename, "\n")
cat("Time since this file was modified: \n")
Sys.time() - file.info(paste0("Data/", filename))$mtime 

# Info
cat("\nIf you want to read a different file, input a different name for 'filename' \n")

# Read data
dat_all <- readRDS(paste0("Data/", filename))

# We save the date part of the text (e.g., '2020-04-23')
# This will be used in part 10, when we save the resulting file
file_date <- substr(filename, 18, 27)    # pick out the part of the text from character no. 17 to no. 26

```

### b. Definition of sum parameters in excel from onedrive  

```{r}

lookup_sums_onedrive <- readxl::read_excel("Input_data/Lookup_tables/EQS summary compounds biota.xlsx")
pars2 <- lookup_sums_onedrive$`Abbr MGR`

```


## 3. Info and exploration 

### Existing sum parameters  
```{r}

pars1 <- dat_all %>% distinct(PARAM) %>% pull(PARAM)

grep("sum", pars1, ignore.case = TRUE, value = TRUE)
grep("PCB", pars1, ignore.case = FALSE, value = TRUE)
grep("loq", pars1, ignore.case = FALSE, value = TRUE)

dat_all %>%
  filter(grepl("CB_S7", PARAM) | grepl("CB_S6", PARAM) | 
           grepl("Sum 6 DIN-PCB", PARAM) | PARAM %in% c("Sum PCB 7", "Sum 7 PCB")) %>%
  count(PARAM, MYEAR) %>%
  ggplot(aes(MYEAR, PARAM, fill = n)) +
  geom_tile()


```
### More plots  

```{r}

plot_from_searchtext("hepta")
plot_from_searchtext("_exloq", ignore.case = FALSE)
plot_from_searchtext("DD", ignore.case = FALSE)
plot_from_searchtext("TPHT", ignore.case = TRUE)
plot_from_searchtext("NP", ignore.case = FALSE)
plot_from_searchtext("OP", ignore.case = FALSE)

```

## 4. Delete old sums and add new ones  

### Delete  
```{r}

# Get names of old ones  
sumpars_exloq <- grep("_exloq", pars1, value = TRUE)
sumpars <- c(
  sub("_exloq", "", sumpars_exloq),
  sumpars_exloq
)
sumpars

dat_sums_deleted <- dat_all %>%
  filter(!PARAM %in% sumpars)

message("Rows before deletion: ", nrow(dat_all))
message("Rows after deletion: ", nrow(dat_sums_deleted))

```

### Add new sums  

#### Test 1   
```{r}

table(dat_sums_deleted$STATION_CODE) %>% sort()

testdata1 <- dat_sums_deleted %>%
  filter(
    PARAM %in% sum_parameters[["sumPCB7"]],
    STATION_CODE %in% c("30A", "30B")
  )
  

testdata2 <- add_sumparameter_inclloq_allyears(1, sum_parameters, testdata1)
testdata3 <- add_sumparameter_exloq_allyears(1, sum_parameters, testdata1)

nrow(testdata1)
nrow(testdata2)
nrow(testdata3)

ggplot(testdata2, aes(MYEAR, VALUE))

table(testdata1$PARAM)
table(testdata2$PARAM)
table(testdata3$PARAM)

xtabs(~PARAM + addNA(FLAG1), testdata1)
xtabs(~PARAM + addNA(FLAG1), testdata2)
xtabs(~PARAM + addNA(FLAG1), testdata3)

xtabs(~PARAM + addNA(N_par), testdata1)
xtabs(~PARAM + addNA(N_par), testdata2)
xtabs(~PARAM + addNA(N_par), testdata3)


# test column names
# add_sumparameter_inclloq_allyears adds only 'n_under_loq' to the data
test_columns <- data.frame(x = names(testdata1), data1 = TRUE) %>%
  full_join(data.frame(x = names(testdata2), data2 = TRUE)) %>%
  full_join(data.frame(x = names(testdata3), data3 = TRUE))

# View(test_columns)

```

### Test 2 . manual check  

```{r}

check <- testdata2 %>%
  filter(MYEAR == 1990 & STATION_CODE == "30A" & SAMPLE_NO2 == 2)
# View(check)

```

### Test 3

```{r}

testdata2 <- add_sumparameter_inclloq_allyears(1, sum_parameters, data = testdata1)   # add new rows to testdata1
testdata2 <- add_sumparameter_exloq_allyears(1, sum_parameters, data = testdata2)     # add new rows to testdata2

ggplot(testdata2, aes(MYEAR, VALUE_WW, colour = is.na(FLAG1))) +
  geom_point() +
  facet_grid(vars(STATION_CODE), vars(PARAM), scales = "free_y")

```


### Add sums for entire data set  
```{r}

# Initialise new data set
dat_all_new <- dat_sums_deleted

message("Number of rows before adding: ", nrow(dat_all_new))
                                                 
# Add sums including LOQ
for (i in length(sum_parameters))
  dat_all_new <- add_sumparameter_inclloq_allyears(1, sum_parameters, data = dat_all_new)   # add new rows
message("Number of rows after adding sums including LOQ: ", nrow(dat_all_new))

# Add sums excluding LOQ
for (i in length(sum_parameters))
  dat_all_new <- add_sumparameter_exloq_allyears(1, sum_parameters, data = dat_all_new)   # add new rows
message("Number of rows after adding sums excluding LOQ: ", nrow(dat_all_new))


```

## 5. Checking  

### Check for duplicates   

(11-12 seconds)

```{r}

# if (!exists("df_duplicates_all")){    # recalculate only if it doesn't exist (for testing)

df_duplicates <- dat_all_new %>%
  add_count(STATION_CODE, LATIN_NAME, TISSUE_NAME, MYEAR, SAMPLE_NO2, PARAM) %>%
  filter(n > 1)

#}

if (nrow(df_duplicates) > 0){
  xtabs(~PARAM, df_duplicates) %>% print()
  xtabs(~MYEAR, df_duplicates) %>% print()
  xtabs(~MYEAR + PARAM, df_duplicates) %>% print()
  stop("Duplicates in the data! Check 'df_duplicates'. (Section 5)\n")
} else {
  cat("No duplicates found in the data. \n")
}
  

```

## 6. Save the data for later use    
- We use the date the data was downloaded (in script 080) in the filename   
- Used downstream (Script 105) 
- Also downloaded to `C:\Data\seksjon 212\Milkys2_pc\Files_from_Jupyterhub_2020\Raw_data` and used for ICES submission  
```{r}

# We make a file name using 'file_date' extracted from the original file (see part 2a)
filename <- paste0("Data/103_data_updated_", file_date, ".rds")

# Save in R format
saveRDS(dat_all_new, filename)

# Save in csv format
filename_csv <- sub("rds$", "csv", filename)
write_csv(dat_all_new, filename_csv)

# To read this data, we use a sentence such as 
#   data_updated2 <- readRDS(filename)    


cat("Updated and standardized data saved as:")
cat("\n", filename, "\n")

```
